spark-shell \
  --conf spark.hadoop.fs.s3a.endpoint=http://localstack:4566 \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.access.key=dummy \
  --conf spark.hadoop.fs.s3a.secret.key=dummy \
  --conf spark.hadoop.fs.s3a.path.style.access=true


configuração airflow
{"host": "http://localstack:4566", "aws_access_key_id": "dummy", "aws_secret_access_key": "dummy", "region_name": "eu-west-1"}